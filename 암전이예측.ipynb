{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "암전이예측.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HIsu1231/Cancer_metastasis_prediction_project/blob/main/%EC%95%94%EC%A0%84%EC%9D%B4%EC%98%88%EC%B8%A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToAvYMCNQmM7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YXadNYgCjGSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b119148-35cd-4c90-8b6f-7a37da63c8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "_IR-fge2lOI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, df_dir):\n",
        "    self.df = pd.read_csv(df_dir)\n",
        "    self.l = []\n",
        "    for i in range(int(len(self.df.columns)/3)):\n",
        "      for j in range(0,len(self.df),len(self.df)):\n",
        "        self.l.append([self.df.iloc[j, i*3:3*i+3]])\n",
        "                                            \n",
        "  def __len__(self):                                                                                  \n",
        "    return  len(self.l)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    inp = np.array(self.df.iloc[1:, idx*3:3*idx+3])\n",
        "    inp = inp.astype(np.float32)\n",
        "    label = np.array(self.df.iloc[0, idx*3])\n",
        "    label = label.astype(np.float32)\n",
        "    return inp, label"
      ],
      "metadata": {
        "id": "tKs7mlS0Qojz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset('/content/drive/MyDrive/전처리데이터_4차_추가_제거.csv')"
      ],
      "metadata": {
        "id": "4Ju79TEj22Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = len(dataset)\n",
        "\n",
        "train_size = int(dataset_size * 0.8)\n",
        "test_size = dataset_size-train_size\n",
        "\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset,[train_size, test_size])"
      ],
      "metadata": {
        "id": "8tPS-PGvW8vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MAFOcl4bjVow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4)"
      ],
      "metadata": {
        "id": "RjaG4zmnQ3kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, samples in enumerate(train_dataloader):\n",
        "  x_data, y_data = samples\n",
        "  print(batch_idx, x_data, y_data)\n",
        "  break"
      ],
      "metadata": {
        "id": "xDL2HxhMW3ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580f6fc5-9411-4456-8099-be819ce5a824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor([[[2.0020e+05, 6.4000e+00, 4.0000e-01],\n",
            "         [2.0021e+05, 5.7000e+00, 9.0000e-01],\n",
            "         [2.0021e+05, 4.7000e+00, 1.6000e+00],\n",
            "         ...,\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[2.1113e+05, 5.0000e+00, 0.0000e+00],\n",
            "         [2.1120e+05, 4.9000e+00, 0.0000e+00],\n",
            "         [2.1120e+05, 5.1000e+00, 3.0000e+00],\n",
            "         ...,\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[2.1072e+05, 4.4000e+00, 1.5000e+00],\n",
            "         [2.1080e+05, 2.4000e+00, 0.0000e+00],\n",
            "         [2.1080e+05, 3.3000e+00, 0.0000e+00],\n",
            "         ...,\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[1.1052e+05, 7.9000e+00, 3.0000e+00],\n",
            "         [1.1052e+05, 0.0000e+00, 1.0000e+00],\n",
            "         [1.1052e+05, 2.5000e+00, 0.0000e+00],\n",
            "         ...,\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00]]]) tensor([0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, in_size):\n",
        "    super().__init__()\n",
        "    self.in_size = in_size\n",
        "    self.h_size = 32\n",
        "    self.n_layers = 2\n",
        "\n",
        "    self.lstm = nn.LSTM(self.in_size, self.h_size, self.n_layers, batch_first=True)\n",
        "    self.h_n = None\n",
        "    self.c_n = None\n",
        "\n",
        "    self.linear = nn.Linear(self.h_size, 1)\n",
        "    self.acti = nn.Sigmoid()\n",
        "\n",
        "  def init_states(self, batch_size):\n",
        "    self.h_n = torch.zeros(self.n_layers, batch_size, self.h_size,device=device)\n",
        "    self.c_n = torch.zeros(self.n_layers, batch_size, self.h_size,device=device)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    states = (self.h_n, self.c_n)\n",
        "    out, (self.h_n, self.c_n) = self.lstm(x, states)\n",
        "\n",
        "    out = out[:, -1,:]\n",
        "    out = self.linear(out)\n",
        "    out = self.acti(out)\n",
        "\n",
        "\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "n2GmI3WPWRFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(in_size=3).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "hJq_xTCsMlNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_acc(pred, label):\n",
        "    # pred_flat = pred.cpu().data.numpy().argmax(axis=1).flatten()\n",
        "    # # pred_flat = np.argmax(pred, axis=1).flatten()\n",
        "    # labels_flat = label.flatten()\n",
        "    # # print(pred_flat)\n",
        "    # # print(labels_flat)\n",
        "    # return np.sum(pred_flat == labels_flat)\n",
        "    acc = 0\n",
        "    pred = pred.detach().flatten()\n",
        "    for i in range(len(pred)):\n",
        "        if pred[i] < 0.5 :\n",
        "            pred[i] = 0\n",
        "        else:\n",
        "            pred[i] = 1\n",
        "        if pred[i] == label[i]:\n",
        "            acc += 1\n",
        "    return acc"
      ],
      "metadata": {
        "id": "HHRlH-zJblJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "n_epochs = 50\n",
        "acc=0\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    y_pred = []\n",
        "    y_test = []\n",
        "    # train\n",
        "    i, train_loss, val_loss = 0, 0., 0.\n",
        "    epoch_acc = 0\n",
        "    data_cnt = 0\n",
        "    for x, y in train_dataloader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        model.init_states(batch_size=x.shape[0])\n",
        "        out = model(x)\n",
        "        # print(out.view(-1))\n",
        "        y = y.float().view(-1, 1)\n",
        "        # print('-')\n",
        "        # print(out)\n",
        "        # print(y)\n",
        "        # break\n",
        "        loss = criterion(out, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        i += 1\n",
        "        acc = cal_acc(out, y)\n",
        "        epoch_acc += acc\n",
        "        data_cnt += len(y)\n",
        "    train_loss /= i\n",
        "    epoch_acc /= data_cnt\n",
        "    # validate\n",
        "    with torch.no_grad():\n",
        "        i = 0\n",
        "        for x, y in test_dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            y = y.float().view(-1, 1)\n",
        "            model.init_states(batch_size=x.shape[0])\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            val_loss += loss.item()\n",
        "            i += 1\n",
        "            y_pred.append(out)\n",
        "            y_test.append(y)\n",
        "        val_loss /= i\n",
        "\n",
        "    print('epoch: {} | train_loss: {:0.5f} | val_loss: {:0.5f} | epoch_acc : {}'.format(epoch, train_loss, val_loss, epoch_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74HZI4YvRDAv",
        "outputId": "ca038677-adb5-441a-e5e4-0f8ac743fe18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 | train_loss: 0.59738 | val_loss: 0.50856 | epoch_acc : 0.7829181494661922\n",
            "epoch: 2 | train_loss: 0.54395 | val_loss: 0.50002 | epoch_acc : 0.8042704626334519\n",
            "epoch: 3 | train_loss: 0.54294 | val_loss: 0.50229 | epoch_acc : 0.8042704626334519\n",
            "epoch: 4 | train_loss: 0.54326 | val_loss: 0.50305 | epoch_acc : 0.8042704626334519\n",
            "epoch: 5 | train_loss: 0.54339 | val_loss: 0.50350 | epoch_acc : 0.8042704626334519\n",
            "epoch: 6 | train_loss: 0.54347 | val_loss: 0.50379 | epoch_acc : 0.8042704626334519\n",
            "epoch: 7 | train_loss: 0.54353 | val_loss: 0.50398 | epoch_acc : 0.8042704626334519\n",
            "epoch: 8 | train_loss: 0.54356 | val_loss: 0.50413 | epoch_acc : 0.8042704626334519\n",
            "epoch: 9 | train_loss: 0.54359 | val_loss: 0.50424 | epoch_acc : 0.8042704626334519\n",
            "epoch: 10 | train_loss: 0.54362 | val_loss: 0.50432 | epoch_acc : 0.8042704626334519\n",
            "epoch: 11 | train_loss: 0.54363 | val_loss: 0.50439 | epoch_acc : 0.8042704626334519\n",
            "epoch: 12 | train_loss: 0.54365 | val_loss: 0.50445 | epoch_acc : 0.8042704626334519\n",
            "epoch: 13 | train_loss: 0.54366 | val_loss: 0.50449 | epoch_acc : 0.8042704626334519\n",
            "epoch: 14 | train_loss: 0.54367 | val_loss: 0.50453 | epoch_acc : 0.8042704626334519\n",
            "epoch: 15 | train_loss: 0.54368 | val_loss: 0.50457 | epoch_acc : 0.8042704626334519\n",
            "epoch: 16 | train_loss: 0.54368 | val_loss: 0.50460 | epoch_acc : 0.8042704626334519\n",
            "epoch: 17 | train_loss: 0.54369 | val_loss: 0.50462 | epoch_acc : 0.8042704626334519\n",
            "epoch: 18 | train_loss: 0.54370 | val_loss: 0.50464 | epoch_acc : 0.8042704626334519\n",
            "epoch: 19 | train_loss: 0.54370 | val_loss: 0.50466 | epoch_acc : 0.8042704626334519\n",
            "epoch: 20 | train_loss: 0.54370 | val_loss: 0.50468 | epoch_acc : 0.8042704626334519\n",
            "epoch: 21 | train_loss: 0.54371 | val_loss: 0.50469 | epoch_acc : 0.8042704626334519\n",
            "epoch: 22 | train_loss: 0.54371 | val_loss: 0.50470 | epoch_acc : 0.8042704626334519\n",
            "epoch: 23 | train_loss: 0.54371 | val_loss: 0.50472 | epoch_acc : 0.8042704626334519\n",
            "epoch: 24 | train_loss: 0.54372 | val_loss: 0.50473 | epoch_acc : 0.8042704626334519\n",
            "epoch: 25 | train_loss: 0.54372 | val_loss: 0.50474 | epoch_acc : 0.8042704626334519\n",
            "epoch: 26 | train_loss: 0.54372 | val_loss: 0.50474 | epoch_acc : 0.8042704626334519\n",
            "epoch: 27 | train_loss: 0.54372 | val_loss: 0.50475 | epoch_acc : 0.8042704626334519\n",
            "epoch: 28 | train_loss: 0.54372 | val_loss: 0.50476 | epoch_acc : 0.8042704626334519\n",
            "epoch: 29 | train_loss: 0.54373 | val_loss: 0.50476 | epoch_acc : 0.8042704626334519\n",
            "epoch: 30 | train_loss: 0.54373 | val_loss: 0.50477 | epoch_acc : 0.8042704626334519\n",
            "epoch: 31 | train_loss: 0.54373 | val_loss: 0.50478 | epoch_acc : 0.8042704626334519\n",
            "epoch: 32 | train_loss: 0.54373 | val_loss: 0.50478 | epoch_acc : 0.8042704626334519\n",
            "epoch: 33 | train_loss: 0.54373 | val_loss: 0.50478 | epoch_acc : 0.8042704626334519\n",
            "epoch: 34 | train_loss: 0.54373 | val_loss: 0.50479 | epoch_acc : 0.8042704626334519\n",
            "epoch: 35 | train_loss: 0.54373 | val_loss: 0.50479 | epoch_acc : 0.8042704626334519\n",
            "epoch: 36 | train_loss: 0.54373 | val_loss: 0.50480 | epoch_acc : 0.8042704626334519\n",
            "epoch: 37 | train_loss: 0.54373 | val_loss: 0.50480 | epoch_acc : 0.8042704626334519\n",
            "epoch: 38 | train_loss: 0.54373 | val_loss: 0.50480 | epoch_acc : 0.8042704626334519\n",
            "epoch: 39 | train_loss: 0.54373 | val_loss: 0.50480 | epoch_acc : 0.8042704626334519\n",
            "epoch: 40 | train_loss: 0.54374 | val_loss: 0.50481 | epoch_acc : 0.8042704626334519\n",
            "epoch: 41 | train_loss: 0.54374 | val_loss: 0.50481 | epoch_acc : 0.8042704626334519\n",
            "epoch: 42 | train_loss: 0.54374 | val_loss: 0.50481 | epoch_acc : 0.8042704626334519\n",
            "epoch: 43 | train_loss: 0.54374 | val_loss: 0.50481 | epoch_acc : 0.8042704626334519\n",
            "epoch: 44 | train_loss: 0.54374 | val_loss: 0.50481 | epoch_acc : 0.8042704626334519\n",
            "epoch: 45 | train_loss: 0.54374 | val_loss: 0.50481 | epoch_acc : 0.8042704626334519\n",
            "epoch: 46 | train_loss: 0.54374 | val_loss: 0.50482 | epoch_acc : 0.8042704626334519\n",
            "epoch: 47 | train_loss: 0.54374 | val_loss: 0.50482 | epoch_acc : 0.8042704626334519\n",
            "epoch: 48 | train_loss: 0.54374 | val_loss: 0.50482 | epoch_acc : 0.8042704626334519\n",
            "epoch: 49 | train_loss: 0.54374 | val_loss: 0.50482 | epoch_acc : 0.8042704626334519\n",
            "epoch: 50 | train_loss: 0.54374 | val_loss: 0.50482 | epoch_acc : 0.8042704626334519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "\n",
        "print(recall_score(y_teso, y_so))\n",
        "print(precision_score(y_teso, y_so))\n",
        "print(f1_score(y_teso, y_so))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "og6hSsya200c",
        "outputId": "dd2a8274-965e-4c78-f3f6-c44fd2bb5637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py:150: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  y = np.asarray(y)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py:154: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  y = np.array(y, dtype=object)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py:286: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  y = np.asarray(y)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py:290: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  y = np.asarray(y, dtype=object)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py:150: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  y = np.asarray(y)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py:154: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  y = np.array(y, dtype=object)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py:286: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  y = np.asarray(y)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py:290: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  y = np.asarray(y, dtype=object)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-33ef21bacb1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_teso\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_so\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_teso\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_so\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_teso\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_so\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mrecall_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m         \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m     )\n\u001b[1;32m   1911\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average has to be one of \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: unknown is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out.detach().flatten())\n",
        "aut = out.detach().flatten()\n",
        "print(y)\n",
        "for i in range(len(aut)):\n",
        "    if out[i] < 0.5 :\n",
        "        out[i] = 0\n",
        "    else:\n",
        "        out[i] = 1\n",
        "    if out[i] == y[i]:\n",
        "        acc += 1\n",
        "return acc\n",
        "\n"
      ],
      "metadata": {
        "id": "jHMVVexKqZoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_acc(pred, label):\n",
        "    pred_flat = pred.cpu().data.numpy().argmax(axis=1).flatten()\n",
        "    # pred_flat = np.argmax(pred, axis=1).flatten()\n",
        "    labels_flat = label.flatten()\n",
        "    # print(pred_flat)\n",
        "    # print(labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat)"
      ],
      "metadata": {
        "id": "pFbonghCYljV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HJgI9gnOzQby"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}